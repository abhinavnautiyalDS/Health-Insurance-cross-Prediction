{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "GF8Ens_Soomf",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "RoGjAbkUYoAp",
        "qYpmQ266Yuh3",
        "Seke61FWphqN",
        "b0JNsNcRphqO",
        "rFu4xreNphqO",
        "JMzcOPDDphqR",
        "PVzmfK_Ep1ck",
        "89xtkJwZ18nB",
        "DEeCZHJGRoYW",
        "dxZNSnAUShU9",
        "dJ2tPlVmpsJ0",
        "-jK_YjpMpsJ2",
        "OB4l2ZhMeS1U",
        "4qY1EAkEfxKe",
        "HvGl1hHyA_VK",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Project Name**    - Health-Insurance-Cross-Sell-Prediction\n",
        "##### **Contribution**    - INDIVIDUAL\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our client, an insurance company, offers vehicle insurance to its customers. Now, they need your help to build a model that predicts if last year's policyholders would also be interested in buying vehicle insurance.\n",
        "\n",
        "Insurance works by providing compensation for specific losses, damages, illnesses, or deaths in exchange for regular payments called premiums. For example, you might pay Rs. 2000 each year for health insurance that covers up to Rs. 100,000 in medical costs. The insurance company can afford to cover these costs because, out of many customers paying premiums, only a few will need to use the insurance each year. This spreads the risk among all policyholders.\n",
        "\n",
        "Vehicle insurance works the same way. Customers pay an annual premium, and in case of an accident, the insurance company compensates them up to a certain amount.\n",
        "\n",
        "Predicting which customers might want vehicle insurance helps the company target its communication and improve its business strategy and revenue.\n",
        "\n",
        "To create this prediction model, you have data on demographics (like gender, age, and region), vehicles (age and condition), and policy details (premium and sourcing channel)."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic libaries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#plotting libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\n",
        "\n",
        "#ML models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,roc_curve,precision_score,recall_score,roc_auc_score\n",
        "\n",
        "\n",
        "#Hyperparameter Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "mQENeBrKMbwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/mydata/TRAIN-HEALTH INSURANCE CROSS SELL PREDICTION.csv\")\n",
        "df_copy=df.copy()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"number of rows \",df.shape[0],\"and number of columns\",df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains 381109 number of rows and 12 columns out of which none of the rows are duplicated and no columns have any null values"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- id: Unique identifier for each customer.\n",
        "\n",
        "- Age: Age of the customer, ranging between 20 and 85 years.\n",
        "\n",
        "- Driving_License: Binary variable indicating if the customer has a driving license (0: No, 1: Yes).\n",
        "\n",
        "- Region_Code: Code representing the region of the customer.\n",
        "\n",
        "- Previously_Insured: Binary variable indicating if the customer already has vehicle insurance (0: No, 1: Yes).\n",
        "\n",
        "- Annual_Premium: The annual premium amount paid by the customer. Note: There are some outlier values as indicated by a sudden jump from the 75th percentile value to the maximum value.\n",
        "\n",
        "- Policy_Sales_Channel: Code for the channel through which the policy was sold.\n",
        "\n",
        "- Vintage: Number of days the customer has been with the company since they purchased insurance.\n",
        "\n",
        "- Response: Binary target variable indicating if the customer is interested in vehicle insurance (0: No, 1: Yes)."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gender\n",
        "df.Gender.unique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AGe\n",
        "df.Age.unique()"
      ],
      "metadata": {
        "id": "J4pBwOxWPdy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Driving_License\n",
        "df.Driving_License.unique()"
      ],
      "metadata": {
        "id": "5NTK2u7nPldm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "VxczfA-UPwbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Region_Code\n",
        "df.Region_Code.unique()"
      ],
      "metadata": {
        "id": "GGimSQGqPp6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Previously_Insured\n",
        "df.Previously_Insured.unique()"
      ],
      "metadata": {
        "id": "wlCTIUe1P7HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vehicle_Age\n",
        "df.Vehicle_Age.unique()"
      ],
      "metadata": {
        "id": "UQ7oFkrfP-0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vehicle_Damage\n",
        "df.Vehicle_Damage.unique()"
      ],
      "metadata": {
        "id": "l0we9W0RQCKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Annual_Premium\n",
        "df.Annual_Premium.unique()"
      ],
      "metadata": {
        "id": "gIWEKmmoQGDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Policy_Sales_Channel\n",
        "df.Policy_Sales_Channel.unique()"
      ],
      "metadata": {
        "id": "jiO5RrMjQJav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vintage\n",
        "df.Vintage.unique()"
      ],
      "metadata": {
        "id": "wEBeUPmvQNLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Response\n",
        "df.Response.unique()"
      ],
      "metadata": {
        "id": "xo1Ml_1JQTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "7wcAdfw0QxRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "ol17DmDIQ0P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Annual Prenium**"
      ],
      "metadata": {
        "id": "BG4T6Fuqcihh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()['Annual_Premium']"
      ],
      "metadata": {
        "id": "nZy3ICEURCmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the above observation i can see there are some outliers present in Annual_Premium column.**"
      ],
      "metadata": {
        "id": "pc2jo7C-RqUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=df,x='Annual_Premium')"
      ],
      "metadata": {
        "id": "33lYmicfRVWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling outliers**"
      ],
      "metadata": {
        "id": "W_7HDwYPSJCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1=df.Annual_Premium.quantile(.25)\n",
        "Q3=df.Annual_Premium.quantile(.75)\n",
        "IQR=Q3-Q1\n",
        "min_whisker=Q1-1.5*IQR\n",
        "max_whisker=Q1+1.5*IQR\n",
        "\n",
        "def fixed_outlier(row):\n",
        "  if row>max_whisker:\n",
        "    return Q3\n",
        "  else:\n",
        "    return row\n",
        "\n",
        "df.insert(9,\"Annual_Premium_Fixed\",np.nan)\n",
        "df['Annual_Premium_Fixed']=df['Annual_Premium'].apply(fixed_outlier)"
      ],
      "metadata": {
        "id": "HkMg-TjQSL-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking**"
      ],
      "metadata": {
        "id": "zcEwAe5sUZIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
        "\n",
        "# Plot the original Annual Premium\n",
        "sns.boxplot(ax=ax[0], data=df, y='Annual_Premium')\n",
        "ax[0].set_title('Original Annual Premium')\n",
        "\n",
        "# Plot the fixed Annual Premium\n",
        "sns.boxplot(ax=ax[1], data=df, y='Annual_Premium_Fixed')\n",
        "ax[1].set_title('Fixed Annual Premium')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-y7zjzhnUA69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Age**"
      ],
      "metadata": {
        "id": "_9f3vn6Fcmsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=df,x='Age',kde=True)"
      ],
      "metadata": {
        "id": "L7Zod-PJcQzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a significant dispersion in the Age column, which makes it difficult to gain meaningful insights. To address this, we can convert the Age values into categorical ranges such as 20-40, 40-60, and 60+."
      ],
      "metadata": {
        "id": "jHNjMCjYdZMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(1,'Age_group',np.nan)"
      ],
      "metadata": {
        "id": "j6nWpfjgdY03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age_group']=df['Age'].apply(lambda x : '20-40' if x>=20 and x<40 else ('40-60' if x>=40 and x<60 else '60+'))"
      ],
      "metadata": {
        "id": "vKPyE0IDeb7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking**"
      ],
      "metadata": {
        "id": "p9-X_EVyfTNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Age_group.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "1oRhuRhUfpuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
        "\n",
        "# Plot the Age\n",
        "sns.histplot(ax=ax[0], data=df, x='Age')\n",
        "ax[0].set_title('Age')\n",
        "\n",
        "# Plot the Age_group\n",
        "sns.countplot(ax=ax[1], data=df, x='Age_group')\n",
        "ax[1].set_title('Age_group')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gxpd0PHbfEXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this dataset has no null values and no duplicate rows, there is no need to handle null values or duplicate values. Additionally, the data types of all features are as required.\n",
        "\n",
        "The only adjustment made was handling outliers in the Annual_Premium column. This column contains many outliers or annual premium values that are unrealistic in real-world scenarios. To address this, I handled all those high premium values by setting them equal to the upper whisker value.\n",
        "\n",
        "There is a significant dispersion in the Age column, which makes it difficult to gain meaningful insights. To address this, we can convert the Age values into categorical ranges such as 20-40, 40-60, and 60+."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1  Distribution of Vintage , Annual_Premium_Fixed, Age\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
        "\n",
        "# Color palette\n",
        "palette = sns.color_palette()\n",
        "\n",
        "#Distribution of Vintage\n",
        "sns.histplot(ax=ax[0], x='Vintage', kde=True, data=df, color=palette[0])\n",
        "ax[0].set_title('Distribution of Vintage', fontsize=16, fontweight='bold')\n",
        "ax[0].set_xlabel('Vintage', fontsize=14)\n",
        "ax[0].set_ylabel('Frequency', fontsize=14)\n",
        "ax[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "#Distribution of Annual Prenium\n",
        "sns.histplot(ax=ax[1], x='Annual_Premium_Fixed',kde=True, data=df, color=palette[1])\n",
        "ax[1].set_title('Distribution of Annual Premium', fontsize=16, fontweight='bold')\n",
        "ax[1].set_xlabel('Annual Premium', fontsize=14)\n",
        "ax[1].set_ylabel('Frequency', fontsize=14)\n",
        "ax[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "#Distribution of Age\n",
        "sns.histplot(ax=ax[2], x='Age', kde=True, data=df, color=palette[2])\n",
        "ax[2].set_title('Distribution of Age', fontsize=16, fontweight='bold')\n",
        "ax[2].set_xlabel('Age', fontsize=14)\n",
        "ax[2].set_ylabel('Frequency', fontsize=14)\n",
        "ax[2].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[2].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Tight layout with additional space for titles\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MwcLqeetcICg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms are best for showing the distribution of values in numerical columns. They make it easy to see the shape of the distribution, such as whether it is normal, skewed, bimodal, etc."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution of Vintage**:\n",
        "The distribution appears to be fairly uniform, indicating that the Vintage feature is uniformly distributed across its range. Each bin has a similar frequency, suggesting that each value of Vintage occurs with approximately the same frequency within the dataset.\n",
        "\n",
        "**Distribution of Annual Premium**:\n",
        "The annual premium feature shows a highly skewed distribution with two distinct peaks.\n",
        "The first peak is around the lower end of the premium range, indicating that most policyholders pay a lower premium.\n",
        "The second peak is around the higher end of the premium range, suggesting a smaller but significant group of policyholders pay a much higher premium.\n",
        "\n",
        "**Distribution of Age**:\n",
        "\n",
        "The distribution of Age is multimodal, with several peaks. It starts with a high frequency at younger ages, decreases, then has a few smaller peaks around middle age, and finally tails off towards older ages. This suggests that the dataset contains more younger individuals, with fewer older individuals, and some variations in between."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Distribution of categorical columns"
      ],
      "metadata": {
        "id": "asQi7byIfPUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#columns to plot\n",
        "cols = ['Age_group', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Response']\n",
        "\n",
        "# Create subplots\n",
        "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\n",
        "ax = ax.flatten()\n",
        "\n",
        "\n",
        "palette = sns.color_palette('Set2')\n",
        "\n",
        "def plot_pie(col, ax):\n",
        "    data = df[col].value_counts()\n",
        "    labels = data.index\n",
        "    sizes = data.values\n",
        "\n",
        "    ax.pie(sizes, labels=labels, colors=palette, autopct='%1.1f%%', startangle=90, shadow=True)\n",
        "    ax.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n",
        "    ax.axis('equal')\n",
        "    ax.legend(labels, loc='upper right', fontsize=10, bbox_to_anchor=(1.1, 1))\n",
        "\n",
        "# Plot each column\n",
        "for col, axes in zip(cols, ax):\n",
        "    plot_pie(col, axes)\n",
        "\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YH_RHZoRfR7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "wcpr-wuxfPUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other than bar plots or count plots, pie charts are the best way to represent how the values in a categorical column are distributed."
      ],
      "metadata": {
        "id": "dhXhPYZYfPUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ROO6fHOCfPUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Most policyholders are aged between 20 and 40, with the fewest being 60+\n",
        "- 99.8% of policyholders have a driving license.\n",
        "- Most customers have vehicles that are 1-2 years old or less than 1 year old.\n",
        "- Half of the policyholders have vehicle damages, while the other half do not."
      ],
      "metadata": {
        "id": "P4LrZJ6rfPUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "The data shows that most policyholders are younger adults aged 20-40, while those over 60 are fewer, almost all policyholders, 99.8%, have a driver's license. Many customers own newer vehicles, typically 1-2 years old or less, suggesting they prefer newer models for reliability. Interestingly, about half of the policyholders have reported vehicle damages, showing a balanced risk across the group. These insights help insurance companies better understand their customers' age preferences, vehicle choices, and risk profiles, allowing them to offer more tailored and effective insurance options."
      ],
      "metadata": {
        "id": "ooqeotZsfPUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Relation of age_group with Annual prenium , Response and Gender\n",
        "\n"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a figure with a grid of subplots\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
        "\n",
        "# Define a color palette\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "\n",
        "# Bar plot for Annual Premium by Age Group\n",
        "sns.barplot(ax=ax[0, 0], x='Age_group', y='Annual_Premium_Fixed', data=df, palette=palette)\n",
        "ax[0, 0].set_title('Annual Premium by Age Group', fontsize=14, fontweight='bold')\n",
        "ax[0, 0].set_xlabel('Age Group', fontsize=12)\n",
        "ax[0, 0].set_ylabel('Annual Premium (Fixed)', fontsize=12)\n",
        "ax[0, 0].tick_params(axis='both', which='major', labelsize=10)\n",
        "ax[0, 0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Count plot for Age Group by Response\n",
        "sns.countplot(ax=ax[0, 1], x='Age_group', hue='Response', data=df, palette=palette)\n",
        "ax[0, 1].set_title('Age Group by Response', fontsize=14, fontweight='bold')\n",
        "ax[0, 1].set_xlabel('Age Group', fontsize=12)\n",
        "ax[0, 1].set_ylabel('Count', fontsize=12)\n",
        "ax[0, 1].tick_params(axis='both', which='major', labelsize=10)\n",
        "ax[0, 1].legend(loc='upper left', title='Response', fontsize=10)\n",
        "ax[0, 1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Bar plot for Gender by Response\n",
        "sns.countplot(ax=ax[1, 0], x='Gender', hue='Response', data=df, palette=palette)\n",
        "ax[1, 0].set_title('Gender vs Response', fontsize=14, fontweight='bold')\n",
        "ax[1, 0].set_xlabel('Gender', fontsize=12)\n",
        "ax[1, 0].set_ylabel('count', fontsize=12)\n",
        "ax[1, 0].tick_params(axis='both', which='major', labelsize=10)\n",
        "ax[1, 0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Count plot for Age Group by Gender\n",
        "sns.countplot(ax=ax[1, 1], x='Age_group', hue='Gender', data=df, palette=palette)\n",
        "ax[1, 1].set_title('Age Group by Gender', fontsize=14, fontweight='bold')\n",
        "ax[1, 1].set_xlabel('Age Group', fontsize=12)\n",
        "ax[1, 1].set_ylabel('Count', fontsize=12)\n",
        "ax[1, 1].tick_params(axis='both', which='major', labelsize=10)\n",
        "ax[1, 1].legend(loc='upper left', title='Gender', fontsize=10)\n",
        "ax[1, 1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bar Plot: Best for showing the relationship between a categorical variable and a numerical variable, such as averages or totals across categories.\n",
        "- Count Plot: Best for visualizing the distribution or relationship between two categorical variables, where each bar represents the count of observations in each category."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 60+ age group customers give more anuual prenium in comparison of rest age groups\n",
        "- 20-40 and 40-60 age group of policyholders are more interested to take vehicle insurance\n",
        "\n",
        "- Male policyholder are more than female one\n",
        "- There are more female than male in 20-40 age group,and for rest of the age groups female are lesser in number than man"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart -4 What is the relationship between Vehicle Age and Annual Premium?\n"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "sns.barplot(x='Vehicle_Age', y='Annual_Premium',hue='Response' ,palette=palette,data=df)\n",
        "plt.title('Annual Premium by Vehicle Age',fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Vehicle Age')\n",
        "plt.ylabel('Annual Premium')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A bar plot is the most effective way to illustrate the relationship between a numerical column and a categorical column in a dataset"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The annual premium for a policyholder with a vehicle older than 2 years is higher compared to those with a vehicle less than 2 years old. Additionally, these customers have a higher likelihood of renewing their policies or purchasing additional coverage."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 How does Age correlate with Vehicle Age?"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "\n",
        "sns.boxplot(x='Vehicle_Age', y='Age', data=df,palette=palette)\n",
        "plt.title('Age by Vehicle Age',fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Vehicle Age')\n",
        "plt.ylabel('Age')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplot can also  used to show relationship  between categorical and numerical"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Policyholders with vehicles less than 1 year old have a median age of 25.\n",
        "\n",
        "2. Policyholders around 50 years old generally have vehicles between 1 and 2 years old.\n",
        "\n",
        "3. Individuals aged 55 years or more tend to own vehicles older than 2 years."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6  how vehicle age affects the response rate (interest in Health insurance)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vehicle Age distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "palette = sns.color_palette(\"Set2\")\n",
        "\n",
        "sns.countplot(data=df, x='Vehicle_Age',hue='Response',palette=palette)\n",
        "plt.title('Distribution of Vehicle Ages',fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Vehicle Age')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Those policyholders with vehicles older than 2 years show a higher interest in taking or renewing their insurance, followed by those with vehicles aged between 1 and 2 years."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 How does the policy sales channel affect the number of policies sold?"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "palette=sns.color_palette('Set2')\n",
        "sns.countplot(data=df, x='Policy_Sales_Channel',hue='Response', palette=palette, order=df['Policy_Sales_Channel'].value_counts().index[:20])\n",
        "plt.title('Top 20 Policy Sales Channels',fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Policy Sales Channel')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Sales Channels like 152.0, 26.0, 124.0, and 160.0 are the most active channels, with a majority of active policyholders having taken policies through these channels."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Which regions contribute the most to the customer base?"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Region Code distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "palette=sns.color_palette()\n",
        "sns.countplot(data=df, x='Region_Code',hue='Response', palette=palette, order=df['Region_Code'].value_counts().index[:20])\n",
        "plt.title('Top 20 Regions by Policyholders',fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Region Code')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most policyholders come from region 28.0, followed by regions 8.0, 46.0, and 41.0"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Is there a correlation between Age and Vintage and Age and Region?"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the subplots\n",
        "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 12))\n",
        "\n",
        "# Age vs Vintage\n",
        "sns.lineplot(ax=ax[0], x='Age', y='Vintage', data=df, marker='o', color='b', linewidth=2)\n",
        "ax[0].set_title('Age vs Vintage', fontsize=16, fontweight='bold')\n",
        "ax[0].set_xlabel('Age', fontsize=14)\n",
        "ax[0].set_ylabel('Vintage (Days)', fontsize=14)\n",
        "ax[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Age vs Region_Code\n",
        "sns.lineplot(ax=ax[1], x='Age', y='Region_Code', data=df, marker='o', color='g', linewidth=2)\n",
        "ax[1].set_title('Age vs Region Code', fontsize=16, fontweight='bold')\n",
        "ax[1].set_xlabel('Age', fontsize=14)\n",
        "ax[1].set_ylabel('Region Code', fontsize=14)\n",
        "ax[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WxJpJ04iIbUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line chart is best to check whether two numrical columns are correlated with each other or not"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 how many previously insured people will be insured again? and Is there any relation between Vehicle Damage and Response?"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the subplots\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "palette = sns.color_palette()\n",
        "\n",
        "# Previously_Insured vs Response\n",
        "sns.countplot(ax=ax[0], x='Previously_Insured', hue='Response', data=df, palette=palette)\n",
        "ax[0].set_title('Previously Insured vs Response', fontsize=16, fontweight='bold')\n",
        "ax[0].set_xlabel('Previously Insured', fontsize=14)\n",
        "ax[0].set_ylabel('Count', fontsize=14)\n",
        "ax[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Vehicle_Damage vs Response\n",
        "sns.countplot(ax=ax[1], x='Vehicle_Damage', hue='Response', data=df, palette=palette)\n",
        "ax[1].set_title('Vehicle Damage vs Response', fontsize=16, fontweight='bold')\n",
        "ax[1].set_xlabel('Vehicle Damage', fontsize=14)\n",
        "ax[1].set_ylabel('Count', fontsize=14)\n",
        "ax[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- if a policy holder insured previously then there is very lower chances that he/she will insured again\n",
        "\n",
        "- Those policyholder having vehical damage are more likely to renewed their insurance policy"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 proportion of individuals within each age group who were previously insured and then insured again"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the subplots\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "palette = sns.color_palette()\n",
        "\n",
        "# Previously_Insured vs Response\n",
        "sns.countplot(ax=ax[0], x='Age_group', hue='Previously_Insured', data=df, palette=palette)\n",
        "ax[0].set_title('Age_group vs Previously_Insured', fontsize=16, fontweight='bold')\n",
        "ax[0].set_xlabel('Age_group', fontsize=14)\n",
        "ax[0].set_ylabel('Count', fontsize=14)\n",
        "ax[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Vehicle_Damage vs Response\n",
        "sns.countplot(ax=ax[1], x='Age_group', hue='Response', data=df, palette=palette)\n",
        "ax[1].set_title('Age_group vs Response', fontsize=16, fontweight='bold')\n",
        "ax[1].set_xlabel('Age_group', fontsize=14)\n",
        "ax[1].set_ylabel('Count', fontsize=14)\n",
        "ax[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n03MpTKWgo8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Age Group 20-40:**\n",
        "\n",
        "High Insurance Rate: Most have insurance.\n",
        "Low Response Rate: Less interested in new offers, likely due to already having coverage.\n",
        "\n",
        "**Age Group 40-60:**\n",
        "\n",
        "Balanced Insurance Status: Evenly split between insured and uninsured.\n",
        "Moderate Response Rate: Mixed interest in offers, influenced by varying needs and commitments.\n",
        "\n",
        "**Age Group 60+:**\n",
        "\n",
        "Low Insurance Rate: Few have insurance.\n",
        "Low Response Rate: Least engaged, possibly due to existing coverage or lack of interest."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 is there any relation between vintage and Response ?"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for this first i made categories out of vintage columns\n",
        "df.insert(12,'Vintage_category',np.nan)"
      ],
      "metadata": {
        "id": "uqnv7Xo2s1x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "df['Vintage_category']=df['Vintage'].apply(lambda x:\"short period\"if x>=0 and x<100 else (\"mid period\" if x>=100 and x<200 else \"Long period\"))"
      ],
      "metadata": {
        "id": "WWr4ydwNs8wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "palette = sns.color_palette('Set2')\n",
        "\n",
        "sns.countplot(data=df,x='Vintage_category',hue='Response',palette=palette)\n",
        "plt.title('Vintage category vs Response', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n"
      ],
      "metadata": {
        "id": "mXicoYRXtAwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I have already done some feature engineering, like creating an age group out of the age column and a vintage category out of the vintage column."
      ],
      "metadata": {
        "id": "oKj3G7388Z7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several categorical columns in this dataset such as Gender, Vehicle Age, Vintage Category, and Previously Insured.\n",
        "Encoding involves assigning a numeric value to each category within a feature, facilitating model learning. If a column has more than 3 categories, consider splitting it into multiple columns to enhance the model's ability to discern patterns in the data."
      ],
      "metadata": {
        "id": "_0KHvfmINykT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding Gender_categories\n",
        "df.insert(2,'Gender_Male',df['Gender'].apply(lambda x: 1 if x=='Male' else 0))\n",
        "df.insert(3,'Gender_Female',df['Gender'].apply(lambda x: 1 if x=='Female' else 0))"
      ],
      "metadata": {
        "id": "YNK5SNEPBLZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding Vehicle_age\n",
        "df.insert(9,'Vehicle_age_>2_years',df['Vehicle_Age'].apply(lambda x : 1 if x=='> 2 Years' else 0))\n",
        "df.insert(9,'Vehicle_age_1-2_years',df['Vehicle_Age'].apply(lambda x : 1 if x=='1-2 Year' else 0))\n",
        "df.insert(10,'Vehicle_age_<1_years',df['Vehicle_Age'].apply(lambda x : 1 if x=='< 1 Year' else 0))"
      ],
      "metadata": {
        "id": "-n63WYoRBLZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding Age_group\n",
        "df.insert(1,'AgeBetween20And40',df['Age_group'].apply(lambda x: 1 if x=='20-40' else 0))\n",
        "df.insert(1,'AgeBetween40And60',df['Age_group'].apply(lambda x: 1 if x=='40-60' else 0))\n",
        "df.insert(1,'Age60+',df['Age_group'].apply(lambda x: 1 if x=='60+' else 0))"
      ],
      "metadata": {
        "id": "gdnjCCBkFEBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding Previously_Insured\n",
        "df.insert(19,'Previously_Insured_yes',df['Previously_Insured'].apply(lambda x: 1 if x==1 else 0))\n",
        "df.insert(20,'Previously_Insured_no',df['Previously_Insured'].apply(lambda x: 1 if x==0 else 0))"
      ],
      "metadata": {
        "id": "AIxT0yKkEmVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding Vintage_category\n",
        "df.insert(22,'Long_term_customer',df['Vintage_category'].apply(lambda x: 1 if x=='Long period' else 0))\n",
        "df.insert(23,'mid_term_customer',df['Vintage_category'].apply(lambda x: 1 if x=='mid period' else 0))\n",
        "df.insert(24,'short_term_customer',df['Vintage_category'].apply(lambda x: 1 if x=='short period' else 0))"
      ],
      "metadata": {
        "id": "LXVMfig8jhDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#labelEncoding Categorical column\n",
        "label=LabelEncoder()\n",
        "for col in df.columns:\n",
        "  if df[col].dtype in ['object']:\n",
        "    df[col]=label.fit_transform(df[[col]])"
      ],
      "metadata": {
        "id": "rDnWnL44SumX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "8jorkxQElqqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I manualy did categorical encoding, without using One hot encoding"
      ],
      "metadata": {
        "id": "tZo2uA1jUniS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Droping unwanted columns"
      ],
      "metadata": {
        "id": "q3wMW5muRjvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "before i proceed i am going to drop all unused columns"
      ],
      "metadata": {
        "id": "SzjJ4A1qU5rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_to_drop=['id','Age_group','Gender','Age','Previously_Insured','Vehicle_Age','Vehicle_Damage','Annual_Premium','Vintage_category','Vintage']\n",
        "df.drop(columns=col_to_drop,axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "nFp4_BM6Rhot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. **Inbalanced dataset**"
      ],
      "metadata": {
        "id": "DEeCZHJGRoYW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is inbalanced datset**\n",
        "\n",
        "If for a classification dataset, if the classes in my target feature are not in same proportion than we can say that the dataset is inbalanced."
      ],
      "metadata": {
        "id": "JpHXxZhyzaUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Response.value_counts().plot(kind='bar')"
      ],
      "metadata": {
        "id": "uOstbV7ZRs3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The categories in this target feature are not proportionately represented, which can cause our model to make incorrect predictions and become biased towards the majority class."
      ],
      "metadata": {
        "id": "aoyre6UrRzsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Handling inbalanced dataset**"
      ],
      "metadata": {
        "id": "dxZNSnAUShU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "x=df.drop(columns='Response')\n",
        "y=df['Response']\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote=SMOTE(sampling_strategy='minority')\n",
        "Xs,Ys=smote.fit_resample(x,y)\n"
      ],
      "metadata": {
        "id": "jkKMAliMRDkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df = pd.concat([pd.DataFrame(Xs, columns=x.columns), pd.DataFrame(Ys, columns=['Response'])], axis=1)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking**"
      ],
      "metadata": {
        "id": "k5t0xQnpV2qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the subplots\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "palette = sns.color_palette()\n",
        "\n",
        "# Previously_Insured vs Response\n",
        "sns.countplot(ax=ax[0], x='Response', data=df, palette=palette)\n",
        "ax[0].set_title('Unbalanced dataset', fontsize=16, fontweight='bold')\n",
        "ax[0].set_xlabel('Response', fontsize=14)\n",
        "ax[0].set_ylabel('Count', fontsize=14)\n",
        "ax[0].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "#After apply SMOTE\n",
        "sns.countplot(ax=ax[1], x='Response', data=balanced_df, palette=palette)\n",
        "ax[1].set_title('balanced dataset ', fontsize=16, fontweight='bold')\n",
        "ax[1].set_xlabel('Response', fontsize=14)\n",
        "ax[1].set_ylabel('Count', fontsize=14)\n",
        "ax[1].tick_params(axis='both', which='major', labelsize=12)\n",
        "ax[1].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_WhmPzYaV5VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Handle this inbalanced dataset i use a Famous Technique called SMOTE stands for synthetic minority oversampling technique, this technique only focuses on minority class , it tried to create new minority sample from the nearest samples using Knn technique."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data Scaling**\n",
        "\n",
        "To ensure that all features contribute equally to the model training process, I have utilized techniques to standardize or normalize my dataset. One such technique is Min-Max Scaling, which scales the numerical features to a range between 0 and 1."
      ],
      "metadata": {
        "id": "x-LbZMfAHRsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#before data scaling\n",
        "balanced_df[['Region_Code','Policy_Sales_Channel','Annual_Premium_Fixed']].head()"
      ],
      "metadata": {
        "id": "uY_8561i6tsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "for col in ['Region_Code','Policy_Sales_Channel','Annual_Premium_Fixed']:\n",
        "    balanced_df[col]=scaler.fit_transform(balanced_df[[col]])\n"
      ],
      "metadata": {
        "id": "A1IthMwQHYvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After data scaling\n",
        "balanced_df[['Region_Code','Policy_Sales_Channel','Annual_Premium_Fixed']].head()"
      ],
      "metadata": {
        "id": "8HeNRmxo7Eed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=balanced_df.drop(columns='Response')\n",
        "Y=balanced_df['Response']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)"
      ],
      "metadata": {
        "id": "-4QK5LWvEbNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(balanced_df.corr())"
      ],
      "metadata": {
        "id": "t0zGzTtI8jIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **feature Selection using Lasso Regression**\n",
        "\n",
        "Lasso regression prevents overfitting by adding a penalty term, , to the error function. In logistic regression, this function typically uses the logarithm of the likelihood.\n",
        "\n",
        "One of the key benefits of Lasso regression is its capability for feature selection. By increasing the value of , less important features have their corresponding coefficients shrink towards zero or become exactly zero. This regularization process promotes sparsity in the model, effectively excluding irrelevant features. As a result, the remaining non-zero coefficients indicate the most important features for predicting the target variable."
      ],
      "metadata": {
        "id": "L1iYe9YDPXxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "# Split the data into training and testing sets\n",
        "\n",
        "# Define the range of alpha values for Lasso regression\n",
        "alpha_values = [0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
        "\n",
        "# Initialize a DataFrame to store the coefficients for each alpha value\n",
        "coefficients = pd.DataFrame(index=X.columns)\n",
        "\n",
        "# Loop over the range of alpha values\n",
        "for alpha in alpha_values:\n",
        "    # Initialize and fit the Lasso model with the current alpha value\n",
        "    lasso = Lasso(alpha=alpha)\n",
        "    lasso.fit(X_train, Y_train)\n",
        "\n",
        "    # Store the coefficients in the DataFrame\n",
        "    coefficients[f'alpha:{alpha}'] =lasso.coef_\n",
        "\n",
        "# Display the resulting DataFrame with coefficients\n",
        "coefficients"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#not important features\n",
        "not_imp_feature={'Driving_License','Region_Code','Policy_Sales_Channel'}"
      ],
      "metadata": {
        "id": "b7MvZILrT_uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **SequentialFeatureSelector**"
      ],
      "metadata": {
        "id": "nGXv2MkRP3AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Sequential Feature Selector (SFS) is a type of wrapper method used in feature selection. Wrapper methods evaluate the performance of a model based on different subsets of features, selecting the subset that yields the best performance.\n",
        "\n",
        "In a wrapper method, the process involves creating subsets of the dataset and training a model on each subset. For classification problems, accuracy is typically used as the evaluation metric, while for regression problems, the R score is used. The subset that results in the best model performance is then selected.\n",
        "\n",
        "In Sequential Feature Selection (SFS), the process works as follows:\n",
        "\n",
        "Initial Training: Start by training the model with all available features.\n",
        "\n",
        "Iterative Removal: Iteratively remove one feature at a time, assessing the impact of each removal on the model's performance. The feature whose removal causes the least decrease in performance is considered the least important.\n",
        "\n",
        "Evaluation: Continue this process until only a single feature remains.\n",
        "\n",
        "Performance Comparison: At each iteration, compare the model's accuracy (or R score) for the different subsets of features. The subset that provides the best performance across all iterations is selected."
      ],
      "metadata": {
        "id": "B7VDH0neSPeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df"
      ],
      "metadata": {
        "id": "O5pUIm9eGn7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "SFS=SequentialFeatureSelector(\n",
        "    estimator=DecisionTreeClassifier(),direction='forward',n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "lvCcLmlQP2wy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SFS.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "7kWJkZGWQxUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SFS.get_feature_names_out()"
      ],
      "metadata": {
        "id": "d8nc7uRFtGDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = SFS.get_support()\n",
        "\n",
        "best_features= set(X.columns[selected_features])\n",
        "print('The selected features are:', best_features)"
      ],
      "metadata": {
        "id": "m9eFhcMfRDPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features=set(df.columns[::-1])\n",
        "notimp_feature2 = all_features - best_features\n",
        "print(notimp_feature2)"
      ],
      "metadata": {
        "id": "Kjsbp9W1U0KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature to drop\n",
        "drop_tofeature=not_imp_feature.intersection(notimp_feature2)\n",
        "print(\"Features which are not important \",drop_tofeature)"
      ],
      "metadata": {
        "id": "p5fiU6jNVB7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Droping non important features**"
      ],
      "metadata": {
        "id": "C8Avq6xXVd2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drop_tofeature='Driving_License'"
      ],
      "metadata": {
        "id": "GjjPXBR_EI0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.drop(columns=drop_tofeature,inplace=True)"
      ],
      "metadata": {
        "id": "NxcSB77cVioh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df"
      ],
      "metadata": {
        "id": "wZ_-jpxKsgpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#suffeling rows before splitting\n",
        "balanced_df=balanced_df.sample(668798)\n",
        "\n",
        "#splitting your data\n",
        "X=balanced_df.drop(columns='Response')\n",
        "Y=balanced_df['Response']\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.1, stratify=Y_train, random_state=42)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have split my dataset into three parts: Training set, Validation set, and Testing set, with a splitting ratio of 70:20:10. Now, I am going to train my model using the Training set and then check its performance using the Validation set. After that, I will perform hyperparameter tuning using the Validation set to find the best parameters. Once I find the best parameters, I will retrain my model using the Training set with these optimal parameters and evaluate its performance using evaluation metrics. Finally, I will test how my model performs on the unseen Testing set, which will give me an unbiased assessment of its performance."
      ],
      "metadata": {
        "id": "naZpzz4kRPMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train \",X_train.shape)\n",
        "print(\"Y_train \",X_train.shape)\n",
        "\n",
        "\n",
        "print(\"X_test \",X_test.shape)\n",
        "print(\"Y_test \",X_test.shape)"
      ],
      "metadata": {
        "id": "kLbBdLLW6a6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?\n"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ideal data splitting ratio depends on the size of the dataset and the problem we are solving. If the training set is too small, the model may not be able to find patterns or relationships in the data, leading to underfitting. Therefore, the training set should be large enough to capture the underlying patterns. On the other hand, if the test set is too small, it may not represent the overall distribution of the data, making the accuracy after testing unreliable. Hence, the test set should also be large enough, but not so large that it makes the training set too small, which could again lead to underfitting.\n",
        "\n",
        "Generally, an 80-20 split is used, meaning 80% of the data is used for training and 20% for testing. This ratio is commonly chosen because it tends to give the best results in most cases."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HyperParameter tuning"
      ],
      "metadata": {
        "id": "ww2k4Ro4x_LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I have used two techniques for Hyperparamter Tuning**\n",
        "\n",
        "1. HalvingRandomSearchCV\n",
        "2. RandomizedSearchCV\n"
      ],
      "metadata": {
        "id": "k9KGIVu_HBQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparamter tuning techniques\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "\n",
        "\n",
        "def Hyperparameter_tuning(model,X_train,Y_train,param_grid,tuning_model):\n",
        "  start=time.time()\n",
        "  if tuning_model=='RandomizedSearchCV':\n",
        "    n_iter=int(input(\"n_iter :\"))\n",
        "    cv=int(input(\"Cv :\"))\n",
        "    rf_grid=RandomizedSearchCV(\n",
        "        estimator=model,param_distributions=param_grid,n_iter=n_iter,n_jobs=-1,cv=cv, verbose=1,scoring='accuracy', random_state=42\n",
        "    )\n",
        "\n",
        "    try:\n",
        "      rf_grid.fit(X_train, Y_train)\n",
        "      return f\"\"\"Best parameters by RandomizedSearchCV: {rf_grid.best_params_},\n",
        "      Best cross-validation score: {rf_grid.best_score_}\n",
        "      execution time : {round(time.time()-start)} sec\n",
        "      \"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n",
        "  else:\n",
        "    start=time.time()\n",
        "    hr_grid=HalvingRandomSearchCV(\n",
        "        estimator=model,param_distributions=param_grid,cv=3,n_jobs=-1,factor=3,verbose=1,scoring='accuracy'\n",
        "    )\n",
        "    try:\n",
        "      hr_grid.fit(X_train, Y_train)\n",
        "      return f\"\"\"\n",
        "      Best parameters by HalvingRandomSearchCV: {hr_grid.best_params_},\n",
        "      Best cross-validation score: {hr_grid.best_score_}\n",
        "      execution time : {round(time.time()-start)} sec\n",
        "      \"\"\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\"\n"
      ],
      "metadata": {
        "id": "YfVvcWM2fOqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Evaluation Metrics"
      ],
      "metadata": {
        "id": "qvc3ZeA-zHKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating a classification model, several metrics can be used to assess its performance comprehensively. Below are key metrics and their significance:\n",
        "\n",
        "1. **Accuracy Score**: This metric indicates the proportion of correct predictions made by the model out of the total number of predictions. While accuracy is straightforward and intuitive, it has a significant limitation: it does not provide information about the types of errors the model is making.\n",
        "\n",
        "2. **Confusion Matrix**: The confusion matrix addresses the shortcomings of the accuracy score by detailing the types of errors. It provides a breakdown of the following:\n",
        "\n",
        "3. **True Positive (TP)**: The model correctly predicts a positive class (e.g., predicting cancer when the patient actually has cancer).\n",
        "False Positive (FP): The model incorrectly predicts a positive class (e.g., predicting cancer when the patient does not have cancer).\n",
        "True Negative (TN): The model correctly predicts a negative class (e.g., predicting no cancer when the patient does not have cancer).\n",
        "False Negative (FN): The model incorrectly predicts a negative class (e.g., predicting no cancer when the patient actually has cancer).\n",
        "Ideally, the number of false positives and false negatives should be zero or close to zero.\n",
        "\n",
        "4. **Precision (P)**: This metric measures the proportion of predicted positive cases that are actually positive:\n",
        "\n",
        "       Precision=TP/(TP+FP)\n",
        "Ideally, the precision value should be 1, indicating no false positives.\n",
        "\n",
        "5. **Recall (R)**: Also known as sensitivity, recall measures the proportion of actual positive cases that are correctly classified:\n",
        "\n",
        "Recall=TP/(TP+FN)\n",
        "Ideally, the recall value should be 1, indicating no false negatives.\n",
        "\n",
        "6. **F1 Score**: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both:\n",
        "\n",
        "F1_Score =2 Precision * Recall/(Precision+Recall)\n",
        "\n",
        "\n",
        "7. **ROC AUC**: The Area Under the Receiver Operating Characteristic Curve (ROC AUC) is a comprehensive metric that evaluates the model's ability to distinguish between classes. An AUC of 1 indicates perfect discrimination:"
      ],
      "metadata": {
        "id": "xY1B_XY6WIxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluation_Metrics(model_object,X_test,Y_test):\n",
        "    Y_pred = model_object.predict(X_test)\n",
        "    y_probs = model_object.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    mat = {\n",
        "        'Accuracy Score': accuracy_score(Y_test, Y_pred),\n",
        "        'Recall Score': recall_score(Y_test, Y_pred),\n",
        "        'Precision Score': precision_score(Y_test, Y_pred),\n",
        "        'F1 Score': f1_score(Y_test, Y_pred),\n",
        "        'ROC AUC Score': roc_auc_score(Y_test, y_probs)\n",
        "    }\n",
        "\n",
        "    mat_df = pd.DataFrame.from_dict(mat, orient='index', columns=['Score'])\n",
        "\n",
        "    return visualise(model_object, Y_test, X_test),mat_df"
      ],
      "metadata": {
        "id": "RN23tobUzQgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Roc_curve and confusion matrix**"
      ],
      "metadata": {
        "id": "MYBYis4m1Nes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualise(model_object,X_test , Y_test):\n",
        "    Y_pred = model_object.predict(X_test)\n",
        "    y_probs = model_object.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, thresholds = roc_curve(Y_test, y_probs)\n",
        "    roc_auc = roc_auc_score(Y_test, y_probs)\n",
        "\n",
        "    # Create subplots\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    ax[0].plot(fpr, tpr, color='red', lw=2,linestyle='--',label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    ax[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    ax[0].set_xlim([0.0, 1.0])\n",
        "    ax[0].set_ylim([0.0, 1.05])\n",
        "    ax[0].set_xlabel('False Positive Rate')\n",
        "    ax[0].set_ylabel('True Positive Rate')\n",
        "    ax[0].set_title('Receiver Operating Characteristic (ROC)')\n",
        "    ax[0].legend(loc=\"lower right\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "    labels = np.array([['True Neg: {}\\n({:.2f}%)'.format(conf_matrix[0, 0], conf_matrix_normalized[0, 0] * 100),\n",
        "                        'False Pos: {}\\n({:.2f}%)'.format(conf_matrix[0, 1], conf_matrix_normalized[0, 1] * 100)],\n",
        "                       ['False Neg: {}\\n({:.2f}%)'.format(conf_matrix[1, 0], conf_matrix_normalized[1, 0] * 100),\n",
        "                        'True Pos: {}\\n({:.2f}%)'.format(conf_matrix[1, 1], conf_matrix_normalized[1, 1] * 100)]])\n",
        "\n",
        "    sns.heatmap(conf_matrix, annot=labels, fmt=\"\", cmap='Blues', cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'], ax=ax[1])\n",
        "    ax[1].set_xlabel('Predicted Labels')\n",
        "    ax[1].set_ylabel('True Labels')\n",
        "    ax[1].set_title('Confusion Matrix')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualise(model_object, Y_test, X_test):\n",
        "    Y_pred = model_object.predict(X_test)\n",
        "    y_probs = model_object.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, thresholds = roc_curve(Y_test, y_probs)\n",
        "    roc_auc = roc_auc_score(Y_test, y_probs)\n",
        "\n",
        "    # Create subplots\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    ax[0].plot(fpr, tpr, color='red', lw=2,linestyle='--',label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    ax[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    ax[0].set_xlim([0.0, 1.0])\n",
        "    ax[0].set_ylim([0.0, 1.05])\n",
        "    ax[0].set_xlabel('False Positive Rate')\n",
        "    ax[0].set_ylabel('True Positive Rate')\n",
        "    ax[0].set_title('Receiver Operating Characteristic (ROC)')\n",
        "    ax[0].legend(loc=\"lower right\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "    labels = np.array([['True Neg: {}\\n({:.2f}%)'.format(conf_matrix[0, 0], conf_matrix_normalized[0, 0] * 100),\n",
        "                        'False Pos: {}\\n({:.2f}%)'.format(conf_matrix[0, 1], conf_matrix_normalized[0, 1] * 100)],\n",
        "                       ['False Neg: {}\\n({:.2f}%)'.format(conf_matrix[1, 0], conf_matrix_normalized[1, 0] * 100),\n",
        "                        'True Pos: {}\\n({:.2f}%)'.format(conf_matrix[1, 1], conf_matrix_normalized[1, 1] * 100)]])\n",
        "\n",
        "    sns.heatmap(conf_matrix, annot=labels, fmt=\"\", cmap='Blues', cbar=False,\n",
        "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "                yticklabels=['Actual Negative', 'Actual Positive'], ax=ax[1])\n",
        "    ax[1].set_xlabel('Predicted Labels')\n",
        "    ax[1].set_ylabel('True Labels')\n",
        "    ax[1].set_title('Confusion Matrix')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ON1E2qg31MvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Logistic Regression"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a classification algorithm that, unlike linear regression which aims to find the best-fit line for predicting continuous values, seeks to find the best classification boundary that can accurately classify the data points in a dataset. The goal is to find a decision boundary that maximizes the likelihood of correctly classifying each data point."
      ],
      "metadata": {
        "id": "Bkc8VCezs_gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L=LogisticRegression()\n",
        "L.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "ifj9_8usqbyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrix\n",
        "print(Evaluation_Metrics(L, X_val, Y_val))"
      ],
      "metadata": {
        "id": "tiCGv83Rqlpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters in Logistic Reggression**\n",
        "\n",
        "1. **Regularization Parameter (C)**: This parameter controls the strength of regularization. If the value of C is very low, it means strong regularization. If the value of C is high, it means weak regularization.\n",
        "\n",
        "2. **Penalty**:  Determines the type of regularization to apply: Lasso regression (L1), Ridge regression (L2), or Elastic Net (a combination of L1 and L2).\n",
        "\n"
      ],
      "metadata": {
        "id": "UH1_VIGWRbHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [ 0.1, 0.2, 0.3, 20,40],\n",
        "    'penalty': ['l1', 'l2', 'elasticnet']\n",
        "}\n",
        "print(Hyperparameter_tuning(L,X_val,Y_val,param_grid,'HalvingRandomSearchCV'))\n",
        "print(Hyperparameter_tuning(L,X_val,Y_val,param_grid,'RandomizedSearchCV'))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model with the best paramters\n",
        "#n_iter=30 and cv=5\n",
        "La=LogisticRegression(\n",
        "    penalty='l2',C=40\n",
        ")\n",
        "La.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "4HHSkynmrm1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrix\n",
        "#Evaluation metrics and Roc_curve\n",
        "\n",
        "print(\"Evaluation Before Hyperparameter Tuning \")\n",
        "print(Evaluation_Metrics(L,X_val,Y_val))\n",
        "\n",
        "print(\"Evaluation after Hyperparameter Tuning \")\n",
        "print(Evaluation_Metrics(La,X_val,Y_val))\n",
        "\n",
        "print(\"Evaluating the model's performance on an unseen dataset.\")\n",
        "print(Evaluation_Metrics(La,X_test,Y_test))"
      ],
      "metadata": {
        "id": "wXz51MYxrwJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used RandomizedSearchCV and HalvingRandomSearchCV because they are more time-efficient compared to GridSearchCV. While GridSearchCV exhaustively searches through all possible combinations of the parameters, leading to long execution times, RandomizedSearchCV and HalvingRandomSearchCV use different strategies to reduce computation time.\n",
        "\n",
        "RandomizedSearchCV evaluates a random subset of parameter combinations, quickly covering a broad search space. HalvingRandomSearchCV starts with many parameter combinations and iteratively narrows down the best-performing ones through successive \"halving\" steps."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a slight change in my accuracy score, which may be due to a slight increase in the number of true negative cases."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Decision Tree\n"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised learning algorithm for classification and regression. It splits data into subsets based on feature values, forming a tree structure with decision nodes and leaf nodes representing outcomes. It uses measures like Gini impurity or variance reduction to choose splits. While easy to interpret, it can overfit without proper pruning."
      ],
      "metadata": {
        "id": "vFj2Ma-7QoyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#for model training\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ],
      "metadata": {
        "id": "G0cA__TdXRbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D=DecisionTreeClassifier()\n",
        "D.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "_mfnGRLfqAFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation metrics and Roc_curve\n",
        "print(Evaluation_Metrics(D,X_val,Y_val))"
      ],
      "metadata": {
        "id": "78NPkWj2qGia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters in Dicision Tree**\n",
        "\n",
        "1. **max_depth**:Maximum depth of the tree. Limits the number of levels to prevent overfitting\n",
        "\n",
        "2. **min_samples_split**:Minimum number of samples required to split an internal node. Prevents splits that don't improve the model much.\n",
        "\n",
        "3. **min_samples_leaf**: Minimum number of samples required at a leaf node. Ensures that leaves have enough samples to provide reliable predictions\n",
        "\n",
        "4. **max_features**:Maximum number of features to consider when looking for the best split. Introduces randomness to help prevent overfitting.\n",
        "\n",
        "5. **criterion**: Function to measure the quality of a split (e.g., Gini impurity, entropy for classification; MSE for regression).\n",
        "\n",
        "6. **max_leaf_nodes**: Maximum number of leaf nodes. Limits the complexity of the model.\n",
        "\n",
        "7. **min_impurity_decrease:** Minimum impurity decrease required to split a node. Ensures splits improve the model sufficiently\n"
      ],
      "metadata": {
        "id": "6quCVjSGQsk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using RandomizedSearchCV for hyperparamter Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "\n",
        "param_grid = {\"splitter\":[\"best\",\"random\"],\n",
        "              'criterion':['gini', 'entropy', 'log_loss'],\n",
        "            \"max_depth\" : [None,5,7,9],\n",
        "           \"min_samples_leaf\":[1,2,4,5,7,9,10],\n",
        "           \"min_samples_split\":[1,4,7,10],\n",
        "           \"max_features\":[\"auto\",\"log2\",\"sqrt\",None],\n",
        "           \"max_leaf_nodes\":[None,2,3,30,40],\n",
        "           'random_state':[23]}\n",
        "\n",
        "\n",
        "# Define the estimator\n",
        "d = DecisionTreeClassifier()\n",
        "#model,X_train,Y_train,param_grid,tuning_model\n",
        "# Initialize RandomizedSearchCV\n",
        "print(Hyperparameter_tuning(d,X_train,Y_train,param_grid,'HalvingRandomSearchCV'))\n",
        "print(Hyperparameter_tuning(d,X_train,Y_train,param_grid,'RandomizedSearchCV'))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model with the best paramters\n",
        "#n_iter=30, cv=4\n",
        "Da=DecisionTreeClassifier(\n",
        "  splitter= 'best', random_state= 23, min_samples_split= 7,\n",
        "  min_samples_leaf= 4, max_leaf_nodes= None, max_features= None, max_depth= None, criterion= 'gini'\n",
        ")\n",
        "Da.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "1JoNtJ-YZmkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrix\n",
        "#Evaluation metrics and Roc_curve\n",
        "\n",
        "print(\"Evaluation Before Hyperparameter Tuning \")\n",
        "print(Evaluation_Metrics(D,X_val,Y_val))\n",
        "\n",
        "print(\"Evaluation after Hyperparameter Tuning \")\n",
        "print(Evaluation_Metrics(Da,X_val,Y_val))\n",
        "\n",
        "print(\"Evaluating the model's performance on an unseen dataset.\")\n",
        "print(Evaluation_Metrics(Da,X_test,Y_test))\n"
      ],
      "metadata": {
        "id": "YDPztyzsZ4gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "test=balanced_df[balanced_df['Response']==1].iloc[:,0:-1].sample(1000)\n",
        "a=pd.Series(Da.predict(test))\n",
        "a.value_counts()"
      ],
      "metadata": {
        "id": "UtUlYNiRafHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here is a slight increase in accuracy, which might be due to an increase in the number of true negative cases (correctly identifying negative instances) and a reduction in false positive cases (incorrectly identifying positive instances). The ROC-AUC score (Receiver Operating Characteristic - Area Under the Curve) has increased to 0.95, indicating that the model has a significantly improved ability to distinguish between positive and negative classes."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 RandomForestClassifier"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Random Forest Classifier is an ensemble learning method that operates by constructing multiple decision trees during training. It is essentially a bagging classifier where the base estimator is a DecisionTreeClassifier, hence the term \"forest\" because it is a collection of trees."
      ],
      "metadata": {
        "id": "R4rEosb3T2ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "R=RandomForestClassifier(n_jobs=-1)\n",
        "R.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrix\n",
        "print(Evaluation_Metrics(R, X_val,Y_val))"
      ],
      "metadata": {
        "id": "WX9Yan-Fs8Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters in RandomForestClassifier**\n",
        "\n",
        "1. **n_estimators**: Number of trees in the forest.\n",
        "\n",
        "2. **max_features**: Maximum number of features each tree is allowed to consider for splitting a node.\n",
        "\n",
        "3. **max_depth**: Maximum depth of each tree.\n",
        "\n",
        "4. **min_samples_split**: Minimum number of samples required to split an internal node.\n",
        "\n",
        "5. **min_samples_leaf**: Minimum number of samples required at a leaf node.\n",
        "\n",
        "6. **bootstrap**: Whether bootstrap samples are used when building trees.\n",
        "\n",
        "7. **criterion**: Function to measure the quality of a split (e.g., Gini impurity, entropy)."
      ],
      "metadata": {
        "id": "bP-G1qGPl7hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [None,150],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2,5,6,7,8,9,10],\n",
        "    'min_samples_leaf': [1,2,3,4],\n",
        "    'max_features': [ 'sqrt', 'log2'],\n",
        "\n",
        "}\n",
        "#print(Hyperparameter_tuning(R,X_train,Y_train,param_grid,'HalvingRandomSearchCV'))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Hyperparameter_tuning(R,X_train,Y_train,param_grid,'RandomizedSearchCV'))\n"
      ],
      "metadata": {
        "id": "L2MPSGmrI7oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train my model with best parameters\n",
        "\n",
        "Ra=RandomForestClassifier(\n",
        "    n_estimators=150,min_samples_split=6,min_weight_fraction_leaf=0.0,min_samples_leaf=1,max_features='sqrt',max_depth=None\n",
        ")\n",
        "\n",
        "Ra.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "qrV_31EbweWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation Metrix\n",
        "#Evaluation metrics and Roc_curve\n",
        "\n",
        "print(\"Evaluation Before Hyperparameter Tuning \")\n",
        "print(Evaluation_Metrics(R,X_val,Y_val))\n",
        "\n",
        "print(\"Evaluation after Hyperparameter Tuning \")\n",
        "print(Evaluation_Metrics(Ra,X_val,Y_val))\n",
        "\n",
        "print(\"Evaluating the model's performance on an unseen dataset.\")\n",
        "print(Evaluation_Metrics(Ra,X_test,Y_test))\n"
      ],
      "metadata": {
        "id": "ty_a83ddwvvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "test=balanced_df[balanced_df['Response']==1].iloc[:,0:-1].sample(1000)\n",
        "a=pd.Series(R.predict(test))\n",
        "a.value_counts()"
      ],
      "metadata": {
        "id": "OD5u-SHd05_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "test=balanced_df[balanced_df['Response']==0].iloc[:,0:-1].sample(1000)\n",
        "a=pd.Series(R.predict(test))\n",
        "a.value_counts()"
      ],
      "metadata": {
        "id": "UT3txuIrYMa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here is a slight increase in accuracy, which might be due to an increase in the number of true negative cases (correctly identifying negative instances) and a reduction in false positive cases (incorrectly identifying positive instances)."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results from all the models we trained and evaluated, we can conclude that the Random Forest Classifier is the best model for our dataset. The optimal parameters for this model are {'n_estimators': 150}. Here are its performance metrics:\n",
        "\n",
        "Accuracy Score: ~0.89\n",
        "\n",
        "Precision: 0.88\n",
        "\n",
        "Recall: 0.89\n",
        "\n",
        "F1 Score: 0.88\n",
        "\n",
        "ROC-AUC Score: 0.96\n",
        "\n",
        "Elapsed Time: 789 seconds\n",
        "\n",
        "This model demonstrates high accuracy, a high ROC-AUC score, high precision, and high recall compared to other models, making it the best choice for our data."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feat_imp=pd.DataFrame()\n",
        "feat_imp['columns']=balanced_df.columns[0:balanced_df.columns.get_loc('Response')]\n",
        "feat_imp['feature_importance']=pd.Series(Ra.feature_importances_)"
      ],
      "metadata": {
        "id": "jEVuuqvGYTUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming feat_imp is your DataFrame containing feature importance\n",
        "plt.figure(figsize=(7,7))  # Adjust the figure size for better readability\n",
        "sns.barplot(data=feat_imp, x='columns', y='feature_importance', palette='viridis')  # Use a color palette for better visuals\n",
        "\n",
        "# Rotate x-axis labels to be vertical\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Features', fontsize=14)\n",
        "plt.ylabel('Feature Importance', fontsize=14)\n",
        "plt.title('Feature Importance of Different Features', fontsize=16)\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()  # Adjust layout to ensure everything fits without overlapping\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ERP9e84BYmeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format\n",
        "\n",
        "---\n",
        "\n",
        "for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "gpkNKEo9ue78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the best perfoming model for me was RandomForestClassifier\n",
        "joblib_filename = 'RandomForestModel.joblib'\n",
        "\n",
        "data_to_save={\n",
        "    'model':Ra,\n",
        "    'functions': {\n",
        "        'Evaluation Metrics':Evaluation_Metrics,\n",
        "        'Visualisation':visualise\n",
        "    }\n",
        "}\n",
        "\n",
        "joblib.dump(data_to_save,joblib_filename)\n",
        "print(f\"Model saved to {joblib_filename}\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_data = joblib.load('RandomForestModel.joblib')\n",
        "\n",
        "# Extract the model, plot data, and functions\n",
        "loaded_model = loaded_data['model']\n",
        "loaded_functions = loaded_data['functions']"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "y_pred=loaded_model.predict(X_test)\n",
        "print(loaded_functions['Evaluation Metrics'](loaded_model,X_test,Y_test))"
      ],
      "metadata": {
        "id": "hMSs8wbg3reG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially, we checked our dataset for null values and duplicates. Since there were no null values or duplicates, no further treatment was required. We then performed feature engineering to create new features. Before data processing, we applied feature scaling techniques to normalize the data, ensuring that all features were on the same scale, making it easier for machine learning algorithms to process.\n",
        "\n",
        "Through Exploratory Data Analysis (EDA), we categorized age into three groups: '20-40', '40-60', and '60+'. We also categorized the vintage into 'Short term', 'Mid term', and 'Long term'. To handle the imbalanced dataset, we used the SMOTE (Synthetic Minority Over-sampling Technique).\n",
        "\n",
        "Our analysis revealed that younger customers (aged 20-40) are more interested in vehicle insurance. Additionally, customers with vehicles older than two years and those with damaged vehicles are more likely to be interested in vehicle insurance.\n",
        "\n",
        "For feature selection, we used Lasso regression and sequential forward selection. We found that the feature 'Previously policy insured' was the most important, while 'Driving license' was the least important.\n",
        "\n",
        "We then applied various machine learning algorithms to predict whether a customer would be interested in vehicle insurance. For logistic regression, we achieved an accuracy score of 83% after hyperparameter tuning. Using decision tree classifier and random forest classifier, we achieved accuracy scores of approximately 88% and 89%, respectively, after hyperparameter tuning.\n",
        "\n",
        "Based on these results, we selected the random forest classifier as our best model, with an accuracy score of 89%.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}